#!/usr/bin/env python3
# Copyright (C) 2024 Apple Inc. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
# 1.  Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 2.  Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in the
#     documentation and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS BE LIABLE FOR
# ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

import os
import subprocess
import argparse
import json
import sys

from webkitpy.results.upload import Upload
from webkitpy.results.options import upload_args
from libraries.webkitscmpy.webkitscmpy import Commit
from libraries.webkitscmpy.webkitscmpy import local, log, remote

CHECKERS = ['NoUncountedMemberChecker', 'NoUncheckedPtrMemberChecker', 'RefCntblBaseVirtualDtor', 'UncheckedCallArgsChecker', 'UncheckedLocalVarsChecker',
    'UncountedCallArgsChecker', 'UncountedLambdaCapturesChecker', 'UncountedLocalVarsChecker']
PROJECTS = ['WebCore', 'WebKit']
STATIC_ANALYZER_UNEXPECTED = 'StaticAnalyzerUnexpectedRegressions'
UNEXPECTED_PASS = {'actual': 'PASS', 'expected': 'FAIL'}
UNEXPECTED_FAILURE = {'actual': 'FAIL', 'expected': 'PASS'}
EXPECTED_FAILURE = {'actual': 'FAIL', 'expected': 'FAIL'}


def parser():
    parser = argparse.ArgumentParser(parents=[upload_args()], description='Finds new regressions and fixes between two smart pointer static analysis results')
    parser.add_argument(
        'new_dir',
        help='Path to directory of results from new build'
    )
    parser.add_argument(
        '--archived-dir',
        dest='archived_dir',
        help='Path to directory of previous results for comparison'
    )
    parser.add_argument(
        '--build-output',
        dest='build_output',
        help='Path to the static analyzer output from the new build',
        required='--generate-results-only' in sys.argv
    )
    parser.add_argument(
        '--scan-build-path',
        dest='scan_build',
        help='Path to scan-build executable',
        required='--generate-results-only' in sys.argv
    )
    parser.add_argument(
        '--generate-results-only',
        dest='generate_filtered_results',
        action='store_true',
        default=False
    )
    parser.add_argument(
        '--check-expectations',
        dest='check_expectations',
        action='store_true',
        default=False,
        help='Compare new results to expectations (instead of a previous run)'
    )
    parser.add_argument(
        '--delete-results',
        dest='delete_results',
        action='store_true',
        default=False,
        help='Delete static analyzer results'
    )
    config_options = parser.add_argument_group('Configuration options')
    config_options.add_argument('--architecture')
    config_options.add_argument('--platform')
    config_options.add_argument('--version')
    config_options.add_argument('--version-name')
    config_options.add_argument('--style')
    config_options.add_argument('--sdk')

    return parser.parse_args()


def find_diff(args, file1, file2):
    # Create empty file if the corresponding one doesn't exist - this happens if a checker is added or removed
    if not args.check_expectations:
        if not os.path.exists(file1):
            f = open(file1, 'a')
            f.close()
    if not os.path.exists(file2):
        f = open(file2, 'a')
        f.close()

    with open(file1) as baseline_file, open(file2) as new_file:
        baseline_list = [line for line in baseline_file.read().splitlines() if not line.startswith('//') and line.strip()]  # Remove lines from the copyright
        new_file_list = new_file.read().splitlines()
        # Find new regressions
        diff_new_from_baseline = set(new_file_list) - set(baseline_list)
        # Find fixes
        diff_baseline_from_new = set(baseline_list) - set(new_file_list)

    return set(diff_new_from_baseline), set(diff_baseline_from_new), new_file_list


def create_filtered_results_dir(args, project, result_paths, category='StaticAnalyzerRegressions'):
    # Create symlinks to new issues only so that we can run scan-build to generate new index.html files
    prefix_path = os.path.abspath(f'{args.build_output}/{category}/{project}/StaticAnalyzerReports')
    subprocess.run(['mkdir', '-p', prefix_path])
    for path_to_report in result_paths:
        report = path_to_report.split('/')[-1]
        path_to_report_new = os.path.join(prefix_path, report)
        subprocess.run(['ln', '-s', os.path.abspath(path_to_report), path_to_report_new])
    print('\n')
    path_to_project = f'{args.build_output}/{category}/{project}'
    subprocess.run([args.scan_build, '--generate-index-only', os.path.abspath(path_to_project)])


def compare_project_results_to_expectations(args, new_path, project, upload_only=False):
    unexpected_issues_total = set()
    unexpected_result_paths_total = set()
    unexpected_buggy_files = set()
    unexpected_clean_files = set()
    project_results_passes = {}
    project_results_failures = {}
    project_results_for_upload = {}

    # Compare the list of dirty files to the expectations list of files
    for checker in CHECKERS:
        # Get unexpected clean and buggy files per checker
        buggy_files, clean_files, current_results = find_diff(args, os.path.join(os.path.dirname(__file__), f'../../Source/{project}/SaferCPPExpectations/{checker}Expectations'), f'{new_path}/{project}/{checker}Files')
        unexpected_clean_files.update(clean_files)
        unexpected_buggy_files.update(buggy_files)

        # Get unexpected issues per checker
        unexpected_issues = set()
        unexpected_result_paths = set()

        with open(f'{new_path}/issues_per_file.json') as f:
            issues_per_file = json.load(f)
        for file_name in buggy_files:
            unexpected_issues.update(list(issues_per_file[checker][file_name].keys()))
            unexpected_result_paths.update(list(issues_per_file[checker][file_name].values()))
        unexpected_result_paths_total.update(unexpected_result_paths)
        unexpected_issues_total.update(unexpected_issues)

        if not upload_only:
            # Set up JSON object
            project_results_passes[checker] = list(clean_files)
            project_results_failures[checker] = list(buggy_files)

            # Create sorted files for each unexpected list - these need the .txt to be displayed in browser
            subprocess.run(['mkdir', '-p', f'{args.build_output}/{STATIC_ANALYZER_UNEXPECTED}/{project}'])
            with open(f'{args.build_output}/{STATIC_ANALYZER_UNEXPECTED}/{project}/UnexpectedPasses{checker}.txt', 'a') as f:
                f.write('\n'.join(sorted(clean_files)))
            with open(f'{args.build_output}/{STATIC_ANALYZER_UNEXPECTED}/{project}/UnexpectedFailures{checker}.txt', 'a') as f:
                f.write('\n'.join(sorted(buggy_files)))
            with open(f'{new_path}/{project}/UnexpectedIssues{checker}', 'a') as f:
                f.write('\n'.join(unexpected_issues))

            print(f'\n{checker}:')
            if clean_files or buggy_files or unexpected_issues:
                print(f'    Unexpected passing files: {len(clean_files)}')
                print(f'    Unexpected failing files: {len(buggy_files)}')
                print(f'    Unexpected issues: {len(unexpected_issues)}')
            else:
                print('    No unexpected results')

        for file in current_results:
            if not file in project_results_for_upload.keys():
                project_results_for_upload[file] = {}
            if file in buggy_files:
                project_results_for_upload[file][checker] = UNEXPECTED_FAILURE
            else:
                project_results_for_upload[file][checker] = EXPECTED_FAILURE
        for file in clean_files:
            if not file in project_results_for_upload.keys():
                project_results_for_upload[file] = {}
            project_results_for_upload[file][checker] = UNEXPECTED_PASS

    if unexpected_issues_total and args.scan_build and not upload_only:
        create_filtered_results_dir(args, project, unexpected_result_paths_total, STATIC_ANALYZER_UNEXPECTED)

    if not unexpected_buggy_files and not unexpected_clean_files and not unexpected_issues_total and not upload_only:
        print('No unexpected results!')

    return unexpected_buggy_files, unexpected_clean_files, unexpected_issues_total, project_results_passes, project_results_failures, project_results_for_upload


def compare_project_results_by_run(args, archive_path, new_path, project):
    new_issues_total = set()
    new_files_total = set()
    fixed_issues_total = set()
    fixed_files_total = set()
    unexpected_result_paths_total = set()
    project_results_passes = {}
    project_results_failures = {}

    for checker in CHECKERS:
        _, fixed_issues, _ = find_diff(args, f'{archive_path}/{checker}Issues', f'{new_path}/{project}/{checker}Issues')
        new_files, fixed_files, _ = find_diff(args, f'{archive_path}/{checker}Files', f'{new_path}/{project}/{checker}Files')
        fixed_issues_total.update(fixed_issues)
        fixed_files_total.update(fixed_files)
        new_files_total.update(new_files)
        new_issues = set()
        unexpected_result_paths = set()

        # Get unexpected issues per checker
        with open(f'{new_path}/issues_per_file.json') as f:
            issues_per_file = json.load(f)
        for file_name in new_files:
            new_issues.update(list(issues_per_file[checker][file_name].keys()))
            unexpected_result_paths.update(list(issues_per_file[checker][file_name].values()))
        unexpected_result_paths_total.update(unexpected_result_paths)
        new_issues_total.update(new_issues)

        # JSON
        project_results_passes[checker] = list(fixed_files)
        project_results_failures[checker] = list(new_files)

        print(f'\n{checker}:')
        if fixed_issues or fixed_files or new_issues or new_files:
            print(f'    Issues fixed: {len(fixed_issues)}')
            print(f'    Files fixed: {len(fixed_files)}')
            print(f'    Unexpected issues: {len(new_issues)}')
            print(f'    Unexpected failing files: {len(new_files)}')
        else:
            print('    No unexpected results')

    if new_issues_total and args.scan_build:
        create_filtered_results_dir(args, project, unexpected_result_paths_total, 'StaticAnalyzerRegressions')

    return new_issues_total, new_files_total, fixed_files_total, fixed_issues_total, project_results_passes, project_results_failures


def upload_results(options, results):
    print('\nPreparing to upload results...')

    config = Upload.create_configuration(
        architecture=options.architecture,
        platform=options.platform,
        version=options.version,
        version_name=options.version_name,
        style=options.style,
        sdk=options.sdk
    )

    repo = os.getcwd()
    if repo.startswith(('https://', 'http://')):
        repository = remote.Scm.from_url(repo)
    else:
        try:
            repository = local.Scm.from_path(path=repo,)
        except OSError:
            log.warning("No repository found at '{}'".format(repo))
            repository = None

    commit = repository.commit()
    commit = commit.Encoder().default(commit)
    commit['repository_id'] = 'webkit'

    upload = Upload(
        suite=options.suite or 'safer-cpp-checks',
        configuration=config,
        details=Upload.create_details(options=options),
        commits=[commit],
        run_stats=Upload.create_run_stats(), results=results,
    )

    for url in options.report_urls:
        if not upload.upload(url):
            sys.stderr.write(f'Failed to upload results\n')


def compare_results(args):
    new_issues_total = set()
    new_files_total = set()
    fixed_files_total = set()
    fixed_issues_total = set()
    unexpected_passes_total = set()
    unexpected_failures_total = set()
    unexpected_issues_total = set()
    unexpected_results_data = {'passes': {}, 'failures': {}}
    results_for_upload = {}
    new_path = os.path.abspath(f'{args.new_dir}')
    archive_path = os.path.abspath(f'{args.archived_dir}') if args.archived_dir else ''

    for project in PROJECTS:
        project_results_for_upload = None
        print(f'\n------ {project} ------')
        if args.check_expectations:
            unexpected_failures, unexpected_passes, unexpected_issues, project_results_passes, project_results_failures, project_results_for_upload = compare_project_results_to_expectations(args, new_path, project)
            unexpected_failures_total.update(unexpected_failures)
            unexpected_passes_total.update(unexpected_passes)
            unexpected_issues_total.update(unexpected_issues)
        else:
            path_to_project = f'{archive_path}/{project}'
            new_issues, new_files, fixed_files, fixed_issues, project_results_passes, project_results_failures = compare_project_results_by_run(args, path_to_project, new_path, project)
            new_issues_total.update(new_issues)
            new_files_total.update(new_files)
            fixed_files_total.update(fixed_files)
            fixed_issues_total.update(fixed_issues)
        # JSON
        unexpected_results_data['passes'][project] = project_results_passes
        unexpected_results_data['failures'][project] = project_results_failures

        if args.report_urls:
            path_for_upload = archive_path or new_path
            if not project_results_for_upload:
                print(f'\nChecking against expectations for results to upload for {project}...')
                _, _, _, _, _, project_results_for_upload = compare_project_results_to_expectations(args, path_for_upload, project, upload_only=True)
            results_for_upload[project] = project_results_for_upload

    if args.build_output:
        results_data_file = os.path.abspath(f"{args.build_output}/unexpected_results.json")
        with open(results_data_file, "w") as f:
            results_data_obj = json.dumps(unexpected_results_data, indent=4)
            f.write(results_data_obj)

    print('\n')
    for type, type_total in {
        'new issues': new_issues_total,
        'new files': new_files_total,
        'fixed files': fixed_files_total,
        'fixed issues': fixed_issues_total,
        'unexpected failing files': unexpected_failures_total,
        'unexpected passing files': unexpected_passes_total,
        'unexpected issues': unexpected_issues_total
    }.items():
        if type_total:
            print(f'Total {type}: {len(type_total)}')

    return results_for_upload


def generate_filtered_results(args):
    report_paths = []
    print(f'Generating new results index with results from {args.build_output}...')

    with open(os.path.abspath(f"{args.build_output}/unexpected_results.json"), "r") as f:
        results_data = json.load(f)
    with open(f'{args.new_dir}/issues_per_file.json') as f:
        issues_per_file = json.load(f)

    for project, checkers in results_data.get('failures').items():
        for checker, files in checkers.items():
            for file in files:
                report_paths += [path_to_report for path_to_report in issues_per_file[checker][file].values()]

    create_filtered_results_dir(args, project, report_paths, category='StaticAnalyzerRegressions')


def main():
    args = parser()

    if not args.generate_filtered_results:
        results_for_upload = compare_results(args)
        if args.report_urls:
            upload_results(args, results_for_upload)
    else:
        generate_filtered_results(args)

    # We don't need the full results for EWS runs. Delete full results if option enabled.
    if args.delete_results:
        path_to_delete = os.path.abspath(f'{args.build_output}/StaticAnalyzer')
        print(f'\nDeleting results from {path_to_delete}...')
        subprocess.run(['rm', '-r', path_to_delete])

    return 0


if __name__ == '__main__':
    main()
